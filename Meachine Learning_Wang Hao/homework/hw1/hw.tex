\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[final]{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage[mathscr]{euscript}
\usepackage{graphicx}
\title{Machine Learning, 2023 Spring\\Assignment 1}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.



\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\textcolor{blue}{Problem 1}
If $\mu=0.9$, what is the probability that a sample of 10 marbles will have $\nu \leq 0.1$ ?

[Hints: 1. Use binomial distribution. 2. The answer is a very small number]


\newpage


\textcolor{blue}{Problem 2}
If $\mu=0.9$, use the Hoeffding Inequality to bound the probability that a sample of 10 marbles will have $\nu \leq 0.1$ and compare the answer to the previous exercise.

\newpage

\textcolor{blue}{Problem 3}
We are given a data set $\mathcal{D}$ and of 25 training examples from an unknown target function $f: \mathcal{X} \rightarrow \mathcal{Y}$, where $\mathcal{X}=\mathbb{R}$ and $\mathcal{Y}=\{-1,+1\}$. To learn $f$, we use a simple hypothesis set $\mathcal{H}=\left\{h_{1}, h_{2}\right\}$ and, where $h_{1}$ is the constant $+1$ function and $h_{2}$ is the constant $-1$.

We consider two learning algorithms, $S$ (smart) and $C$ (crazy). $S$ chooses the hypothesis that agrees the most with $\mathcal{D}$ and $C$ chooses the other hypothesis deliberately. Let us see how these algorithms perform out of sample from the deterministic and probabilistic points of view. Assume in the probabilistic view that there is a probability distribution on $\mathcal{X}$, and let $\mathbb{P}[f(x)=+1]=p$.

(a) Can $\mathrm{S}$ produce a hypothesis that is guaranteed to perform better than random on any point outside $\mathcal{D}$ ?

(b) Assume for the rest of the exercise that all the examples in $\mathcal{D}$ have $y_{n}=+1$. Is it possible that the $C$ hypothesis that produces turns out to be better than the hypothesis that $S$ produces? 

(c) If $p=0.9$, what is the probability that $S$ will produce a better hypothesis than $C ?$

(d) Is there any value of $p$ for which it is more likely than not that $C$ will produce a better hypothesis than $S$ ?




\newpage

\textcolor{blue}{Problem 4}
A friend comes to you with a learning problem. She says the target function  $f$ is completely unknown, but she has 4,000 data points. She is willing to pay you to solve her problem and produce for her a $g$ which approximate $f$. What is the best that you can promise her among the following:

(a) After learning you will provide her with a $g$ that you will guarantee approximates $f$ well out of sample.

(b) After learning you will provide her with a $g$, and with high probability the $g$ which you produce will approximate $f$ well out of sample.

(c) One of two things will happen.

\quad(i) You will produce a hypothesis $g$;

\quad(ii) You will declare that you failed. 

\quad If you do return a hypothesis $g$, then with high probability the $g$ which you produce will approximate $f$ well out of sample.



\newpage



\textcolor{blue}{Problem 5}

Given target function $ f(x) = ax $ ($a$ unknown), now we have the dataset $\{x_1,y_1\}$ ... $\{x_n,y_n\}$. Which hypothesis class will you chose, $ H = \{ax + b\} $ or $H = \{ax\} $, explain your reason.


Given that your target function is $f(x) = ax$, the hypothesis class that directly aligns with this is $H = \{ax\}$. This is because the target function and this hypothesis class both represent linear functions that pass through the origin, with 'a' being the slope or rate of change.

Choosing the $H = \{ax + b\}$ hypothesis class, where 'b' represents the y-intercept, would introduce an additional degree of freedom that is not present in the target function. In the context of machine learning, this could potentially lead to overfitting the data, as it adds an additional parameter that needs to be estimated but is not part of the original target function. 

However, there might be circumstances where you would choose $H = \{ax + b\}$ over $H = \{ax\}$. For example, if you have reason to believe that the target function is incorrect, or incomplete, or if there's evidence in your dataset that a non-zero intercept would provide a better fit, then $H = \{ax + b\}$ might be a better choice. But without such specific circumstances or additional information, $H = \{ax\}$ would generally be the more appropriate choice for your given target function.


\end{document}